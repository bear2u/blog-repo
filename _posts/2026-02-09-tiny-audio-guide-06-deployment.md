---
layout: post
title: "Tiny Audio ì™„ë²½ ê°€ì´ë“œ (06) - ë°°í¬ ë° í™•ì¥"
date: 2026-02-09
permalink: /tiny-audio-guide-06-deployment/
author: Alex Kroman
categories: [ë¨¸ì‹ ëŸ¬ë‹, ìŒì„±ì¸ì‹]
tags: [ASR, Speech Recognition, GLM-ASR, Qwen3, PyTorch, HuggingFace, Audio ML]
original_url: "https://github.com/alexkroman/tiny-audio"
excerpt: "í›ˆë ¨ëœ ëª¨ë¸ì„ HuggingFace Hubì— ë°°í¬í•˜ê³  í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì„ ì•Œì•„ë´…ë‹ˆë‹¤."
---

## HuggingFace Hubì— í‘¸ì‹œ

í›ˆë ¨ëœ ëª¨ë¸ì„ HuggingFace Hubì— ì—…ë¡œë“œí•˜ì—¬ ê³µìœ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ê¸°ë³¸ í‘¸ì‹œ

```bash
# HuggingFace ë¡œê·¸ì¸
huggingface-cli login

# ëª¨ë¸ í‘¸ì‹œ
ta push \
    --checkpoint outputs/transcription/checkpoints/final.pt \
    --repo-id your-username/tiny-audio-custom \
    --private
```

ì‹¤í–‰ ê³¼ì •:

```
[INFO] Preparing model for upload...
[INFO] Converting checkpoint to HuggingFace format...
[INFO] Creating model card...
[INFO] Uploading to your-username/tiny-audio-custom...

Uploading files:
  config.json               â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% 2.3 KB
  pytorch_model.bin         â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% 48.7 MB
  tokenizer_config.json     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% 1.2 KB
  README.md                 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% 4.5 KB
  training_args.json        â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% 3.1 KB

[INFO] Upload successful!
[INFO] Model available at: https://huggingface.co/your-username/tiny-audio-custom
```

### ëª¨ë¸ ì¹´ë“œ ìë™ ìƒì„±

`README.md`ê°€ ìë™ìœ¼ë¡œ ìƒì„±ë©ë‹ˆë‹¤:

```markdown
---
language: en
license: mit
tags:
  - audio
  - automatic-speech-recognition
  - tiny-audio
datasets:
  - librispeech
  - common_voice
metrics:
  - wer
model-index:
  - name: tiny-audio-custom
    results:
      - task:
          type: automatic-speech-recognition
          name: Speech Recognition
        dataset:
          name: LibriSpeech test-clean
          type: librispeech_asr
        metrics:
          - type: wer
            value: 5.2
            name: Word Error Rate
---

# Tiny Audio Custom

This is a custom ASR model trained with Tiny Audio framework.

## Model Description

- **Base Model**: GLM-ASR + Qwen3-0.6B
- **Projector Type**: MLP
- **Training Data**: Multi-ASR dataset
- **Training Steps**: 50,000
- **Training Time**: 24 hours on A40 GPU

## Performance

| Dataset | WER | CER |
|---------|-----|-----|
| LibriSpeech test-clean | 5.2% | 2.1% |
| LibriSpeech test-other | 12.3% | 5.8% |
| Common Voice test | 8.7% | 3.9% |

## Usage

```python
from transformers import pipeline

pipe = pipeline(
    "automatic-speech-recognition",
    model="your-username/tiny-audio-custom",
    trust_remote_code=True
)

result = pipe("audio.wav")
print(result["text"])
```

## Training Details

- Learning rate: 5e-4
- Batch size: 16
- Optimizer: AdamW
- Mixed precision: FP16

## Citation

```bibtex
@misc{tiny-audio-custom,
  author = {Your Name},
  title = {Tiny Audio Custom Model},
  year = {2026},
  publisher = {HuggingFace},
  url = {https://huggingface.co/your-username/tiny-audio-custom}
}
```
```

### ì»¤ìŠ¤í…€ ëª¨ë¸ ì¹´ë“œ

ìì²´ ëª¨ë¸ ì¹´ë“œë¥¼ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```bash
ta push \
    --checkpoint outputs/transcription/checkpoints/final.pt \
    --repo-id your-username/tiny-audio-medical \
    --model-card custom_model_card.md \
    --tags "medical,healthcare,asr"
```

### ë¹„ê³µê°œ ëª¨ë¸

```bash
# Private repositoryë¡œ ì—…ë¡œë“œ
ta push \
    --checkpoint outputs/transcription/checkpoints/final.pt \
    --repo-id your-username/tiny-audio-private \
    --private

# ì‚¬ìš© ì‹œ í† í° í•„ìš”
from transformers import pipeline

pipe = pipeline(
    "automatic-speech-recognition",
    model="your-username/tiny-audio-private",
    use_auth_token="hf_xxxxxxxxxxxxx",
    trust_remote_code=True
)
```

### ì¡°ì§ ê³„ì •ì— í‘¸ì‹œ

```bash
ta push \
    --checkpoint outputs/transcription/checkpoints/final.pt \
    --repo-id your-org/tiny-audio-enterprise \
    --organization your-org
```

## HuggingFace Space ë°°í¬

Gradio ë°ëª¨ë¥¼ HuggingFace Spaceì— ë°°í¬:

### ê¸°ë³¸ ë°°í¬

```bash
ta deploy \
    --checkpoint outputs/transcription/checkpoints/final.pt \
    --space-id your-username/tiny-audio-demo
```

ì‹¤í–‰ ê³¼ì •:

```
[INFO] Creating HuggingFace Space...
[INFO] Preparing demo application...
[INFO] Uploading files...

Files uploaded:
  app.py                    â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% 8.2 KB
  requirements.txt          â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% 0.5 KB
  README.md                 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% 3.1 KB
  model/                    â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% 48.7 MB

[INFO] Building Space... (this may take 5-10 minutes)
[INFO] Space is ready!
[INFO] URL: https://huggingface.co/spaces/your-username/tiny-audio-demo
```

ìƒì„±ëœ `app.py`:

```python
import gradio as gr
from transformers import pipeline

# ëª¨ë¸ ë¡œë“œ
pipe = pipeline(
    "automatic-speech-recognition",
    model="your-username/tiny-audio-custom",
    trust_remote_code=True
)

def transcribe(audio):
    """ì˜¤ë””ì˜¤ ì „ì‚¬"""
    if audio is None:
        return "Please upload an audio file."

    result = pipe(audio, return_timestamps="word")

    # ì „ì²´ í…ìŠ¤íŠ¸
    full_text = result["text"]

    # Word-level timestamps
    timestamps = "\n".join([
        f"{chunk['text']}: {chunk['timestamp'][0]:.2f}s - {chunk['timestamp'][1]:.2f}s"
        for chunk in result["chunks"]
    ])

    return full_text, timestamps

# Gradio ì¸í„°í˜ì´ìŠ¤
demo = gr.Interface(
    fn=transcribe,
    inputs=gr.Audio(type="filepath", label="Upload Audio"),
    outputs=[
        gr.Textbox(label="Transcription"),
        gr.Textbox(label="Word Timestamps", lines=10)
    ],
    title="Tiny Audio Demo",
    description="Upload an audio file to get transcription with word-level timestamps.",
    examples=[
        ["examples/sample1.wav"],
        ["examples/sample2.wav"],
    ],
    cache_examples=True
)

if __name__ == "__main__":
    demo.launch()
```

### ì»¤ìŠ¤í…€ UI

ê³ ê¸‰ ë°ëª¨ UI:

```python
import gradio as gr
from transformers import pipeline
import matplotlib.pyplot as plt
import numpy as np

pipe = pipeline(
    "automatic-speech-recognition",
    model="your-username/tiny-audio-custom",
    trust_remote_code=True
)

def transcribe_and_visualize(audio, show_confidence=True):
    """ì˜¤ë””ì˜¤ ì „ì‚¬ ë° ì‹œê°í™”"""
    if audio is None:
        return None, None, None

    # ì „ì‚¬
    result = pipe(audio, return_timestamps="word")

    # í…ìŠ¤íŠ¸
    text = result["text"]

    # íƒ€ì„ë¼ì¸ ì‹œê°í™”
    fig, ax = plt.subplots(figsize=(12, 4))

    for i, chunk in enumerate(result["chunks"]):
        start, end = chunk["timestamp"]
        word = chunk["text"]

        # ë§‰ëŒ€ ê·¸ë˜í”„
        ax.barh(0, end - start, left=start, height=0.5, alpha=0.7)

        # í…ìŠ¤íŠ¸ ë ˆì´ë¸”
        ax.text(
            (start + end) / 2, 0,
            word,
            ha='center', va='center',
            fontsize=8,
            rotation=45
        )

    ax.set_ylim(-1, 1)
    ax.set_xlabel('Time (seconds)')
    ax.set_title('Word Timeline')
    ax.grid(axis='x', alpha=0.3)

    # í¬ë§·íŒ…ëœ íƒ€ì„ìŠ¤íƒ¬í”„
    timestamps_html = "<div style='font-family: monospace;'>"
    for chunk in result["chunks"]:
        start, end = chunk["timestamp"]
        word = chunk["text"]
        timestamps_html += f"<div>{word:20s} {start:6.2f}s - {end:6.2f}s</div>"
    timestamps_html += "</div>"

    return text, fig, timestamps_html

# Gradio ë¸”ë¡ API
with gr.Blocks(theme=gr.themes.Soft()) as demo:
    gr.Markdown("# Tiny Audio Transcription Demo")
    gr.Markdown("Upload an audio file to get high-quality transcription with word-level timestamps.")

    with gr.Row():
        with gr.Column():
            audio_input = gr.Audio(type="filepath", label="Upload Audio")
            submit_btn = gr.Button("Transcribe", variant="primary")

            gr.Markdown("### Examples")
            gr.Examples(
                examples=[
                    ["examples/sample1.wav"],
                    ["examples/sample2.wav"],
                    ["examples/sample3.wav"],
                ],
                inputs=audio_input
            )

        with gr.Column():
            text_output = gr.Textbox(label="Transcription", lines=3)
            timeline_plot = gr.Plot(label="Word Timeline")
            timestamps_output = gr.HTML(label="Detailed Timestamps")

    submit_btn.click(
        transcribe_and_visualize,
        inputs=[audio_input],
        outputs=[text_output, timeline_plot, timestamps_output]
    )

demo.launch()
```

### GPU Space

GPUê°€ í•„ìš”í•œ ê²½ìš°:

```yaml
# README.mdì— ì¶”ê°€
---
title: Tiny Audio Demo
emoji: ğŸ¤
colorFrom: blue
colorTo: purple
sdk: gradio
sdk_version: 4.0.0
app_file: app.py
pinned: false
license: mit
duplicated_from: your-username/tiny-audio-demo
hardware: t4-small  # or t4-medium, a10g-small, a10g-large
---
```

## Gradio ë°ëª¨ ì‹¤í–‰

ë¡œì»¬ì—ì„œ ë°ëª¨ ì‹¤í–‰:

### ê¸°ë³¸ ë°ëª¨

```bash
ta demo \
    --checkpoint outputs/transcription/checkpoints/final.pt \
    --port 7860
```

ì¶œë ¥:

```
[INFO] Loading model...
[INFO] Creating Gradio interface...
[INFO] Starting demo server...

Running on local URL:  http://127.0.0.1:7860
Running on public URL: https://xxxxx.gradio.live  (expires in 72 hours)

To create a permanent demo, deploy to HuggingFace Spaces.
```

### ê³µìœ  ë§í¬ ìƒì„±

```bash
ta demo \
    --checkpoint outputs/transcription/checkpoints/final.pt \
    --share  # ê³µê°œ URL ìƒì„±
```

### ì¸ì¦ ì¶”ê°€

```bash
ta demo \
    --checkpoint outputs/transcription/checkpoints/final.pt \
    --auth username:password
```

### ì»¤ìŠ¤í…€ ì˜ˆì œ

```bash
ta demo \
    --checkpoint outputs/transcription/checkpoints/final.pt \
    --examples examples/ \
    --cache-examples
```

## Voice Agent í†µí•©

ì‹¤ì‹œê°„ ìŒì„± ëŒ€í™” ì‹œìŠ¤í…œì— í†µí•©:

### Pipecat-AI í†µí•©

[Pipecat](https://github.com/pipecat-ai/pipecat)ì€ ì‹¤ì‹œê°„ ìŒì„± AI íŒŒì´í”„ë¼ì¸ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.

#### ì„¤ì¹˜

```bash
pip install pipecat-ai[tiny-audio]
```

#### ê¸°ë³¸ ì‚¬ìš©

```python
from pipecat.pipeline import Pipeline
from pipecat.transports import WebRTCTransport
from pipecat.audio import VAD
from pipecat.services import TinyAudioSTT, OpenAILLM, OpenAITTS

# íŒŒì´í”„ë¼ì¸ êµ¬ì„±
pipeline = Pipeline([
    # 1. WebRTCë¡œ ì˜¤ë””ì˜¤ ìˆ˜ì‹ 
    WebRTCTransport(
        room_url="https://your-room.daily.co",
    ),

    # 2. VAD (Voice Activity Detection)
    VAD(
        threshold=0.5,
        min_silence_ms=500
    ),

    # 3. Speech-to-Text (Tiny Audio)
    TinyAudioSTT(
        model="your-username/tiny-audio-custom",
        device="cuda"
    ),

    # 4. LLM ì²˜ë¦¬
    OpenAILLM(
        model="gpt-4",
        system_prompt="You are a helpful assistant."
    ),

    # 5. Text-to-Speech
    OpenAITTS(
        voice="nova"
    )
])

# íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
pipeline.run()
```

#### OpenAI Realtime API í†µí•©

OpenAIì˜ Realtime APIì™€ í•¨ê»˜ ì‚¬ìš©:

```python
from pipecat.services import OpenAIRealtimeSTT, OpenAIRealtimeLLM

# OpenAI Realtime ëŒ€ì‹  Tiny Audio ì‚¬ìš©
pipeline = Pipeline([
    WebRTCTransport(room_url="https://your-room.daily.co"),
    VAD(threshold=0.5),

    # Tiny Audio STT (OpenAIë³´ë‹¤ ì €ë ´)
    TinyAudioSTT(
        model="your-username/tiny-audio-custom",
        stream_mode=True,  # ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°
        device="cuda"
    ),

    # OpenAI LLM
    OpenAIRealtimeLLM(
        model="gpt-4-realtime",
        instructions="You are a helpful assistant."
    )
])
```

#### ì»¤ìŠ¤í…€ Tiny Audio ì„œë¹„ìŠ¤

```python
from pipecat.services.base import STTService
from tiny_audio.inference import StreamingASRInference
import numpy as np

class TinyAudioSTT(STTService):
    """Pipecatìš© Tiny Audio STT ì„œë¹„ìŠ¤"""

    def __init__(self, model, device="cuda", chunk_size=1600):
        super().__init__()
        self.inference = StreamingASRInference(
            model_name=model,
            device=device,
            chunk_size=chunk_size
        )
        self.buffer = []

    async def process_audio(self, audio_chunk: np.ndarray):
        """ì˜¤ë””ì˜¤ ì²­í¬ ì²˜ë¦¬"""
        # ìŠ¤íŠ¸ë¦¬ë° ì¶”ë¡ 
        partial_text = self.inference.process_chunk(audio_chunk)

        if partial_text:
            # ë¶€ë¶„ ê²°ê³¼ ì „ì†¡
            await self.emit_partial(partial_text)

        return None

    async def finalize(self):
        """ìµœì¢… ê²°ê³¼ ìƒì„±"""
        final_text = self.inference.finalize()

        # ìµœì¢… ê²°ê³¼ ì „ì†¡
        await self.emit_final(final_text)

        # ë²„í¼ ì´ˆê¸°í™”
        self.inference.reset()

        return final_text
```

### WebRTC ìŠ¤íŠ¸ë¦¬ë°

ì‹¤ì‹œê°„ WebRTC ìŒì„± ì „ì†¡:

```python
from aiortc import RTCPeerConnection, RTCSessionDescription
from av import AudioFrame
import asyncio

class TinyAudioWebRTC:
    """WebRTCë¥¼ í†µí•œ ì‹¤ì‹œê°„ ASR"""

    def __init__(self, model):
        self.pc = RTCPeerConnection()
        self.inference = StreamingASRInference(model_name=model)
        self.setup_tracks()

    def setup_tracks(self):
        """ì˜¤ë””ì˜¤ íŠ¸ë™ ì„¤ì •"""

        @self.pc.on("track")
        async def on_track(track):
            """ì˜¤ë””ì˜¤ íŠ¸ë™ ìˆ˜ì‹ """

            if track.kind == "audio":
                while True:
                    try:
                        frame = await track.recv()
                        await self.process_frame(frame)
                    except Exception as e:
                        print(f"Error: {e}")
                        break

    async def process_frame(self, frame: AudioFrame):
        """ì˜¤ë””ì˜¤ í”„ë ˆì„ ì²˜ë¦¬"""
        # ì˜¤ë””ì˜¤ ë°ì´í„° ì¶”ì¶œ
        audio_data = frame.to_ndarray()

        # 16kHzë¡œ ë¦¬ìƒ˜í”Œë§ (í•„ìš”ì‹œ)
        if frame.sample_rate != 16000:
            audio_data = resample(audio_data, frame.sample_rate, 16000)

        # ìŠ¤íŠ¸ë¦¬ë° ì¶”ë¡ 
        result = self.inference.process_chunk(audio_data)

        if result:
            # WebSocketìœ¼ë¡œ ê²°ê³¼ ì „ì†¡
            await self.send_result(result)

    async def send_result(self, text):
        """ê²°ê³¼ ì „ì†¡"""
        # í´ë¼ì´ì–¸íŠ¸ì—ê²Œ ì „ì†¡
        print(f"Transcription: {text}")

# ì‚¬ìš©
webrtc = TinyAudioWebRTC(model="your-username/tiny-audio-custom")

# WebRTC ì—°ê²° ì„¤ì •
offer = await webrtc.pc.createOffer()
await webrtc.pc.setLocalDescription(offer)
```

### VAD (Voice Activity Detection) í†µí•©

ìŒì„± êµ¬ê°„ë§Œ ì „ì‚¬:

```python
import webrtcvad
import numpy as np

class VADWithTinyAudio:
    """VAD + Tiny Audio"""

    def __init__(self, model, vad_aggressiveness=3):
        self.vad = webrtcvad.Vad(vad_aggressiveness)
        self.inference = StreamingASRInference(model_name=model)
        self.is_speaking = False
        self.speech_buffer = []

    def process_audio(self, audio_chunk, sample_rate=16000):
        """ì˜¤ë””ì˜¤ ì²˜ë¦¬ with VAD"""
        # VAD ì²´í¬ (10, 20, 30ms ì²­í¬ë§Œ ì§€ì›)
        frame_duration_ms = 30
        frame_size = int(sample_rate * frame_duration_ms / 1000)

        # ì²­í¬ë¥¼ í”„ë ˆì„ìœ¼ë¡œ ë¶„í• 
        for i in range(0, len(audio_chunk), frame_size):
            frame = audio_chunk[i:i+frame_size]

            if len(frame) < frame_size:
                break

            # Int16ìœ¼ë¡œ ë³€í™˜
            frame_int16 = (frame * 32768).astype(np.int16)

            # VAD ì‹¤í–‰
            is_speech = self.vad.is_speech(
                frame_int16.tobytes(),
                sample_rate
            )

            if is_speech:
                # ìŒì„± ì‹œì‘
                if not self.is_speaking:
                    self.is_speaking = True
                    print("[VAD] Speech started")

                # ë²„í¼ì— ì¶”ê°€
                self.speech_buffer.append(frame)

            elif self.is_speaking:
                # ìŒì„± ì¢…ë£Œ
                self.is_speaking = False
                print("[VAD] Speech ended")

                # ì „ì²´ ìŒì„± êµ¬ê°„ ì „ì‚¬
                full_audio = np.concatenate(self.speech_buffer)
                result = self.inference.process_chunk(full_audio)

                # ë²„í¼ ì´ˆê¸°í™”
                self.speech_buffer = []

                return result

        return None

# ì‚¬ìš©
vad_asr = VADWithTinyAudio(
    model="your-username/tiny-audio-custom",
    vad_aggressiveness=3  # 0-3, ë†’ì„ìˆ˜ë¡ ì—„ê²©
)

# ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬
for chunk in audio_stream:
    result = vad_asr.process_audio(chunk)
    if result:
        print(f"Transcription: {result}")
```

## ì»¤ìŠ¤í…€ Projector ì¶”ê°€

ìƒˆë¡œìš´ Projector ì•„í‚¤í…ì²˜ë¥¼ ì¶”ê°€í•˜ëŠ” ë°©ë²•:

### 1. Projector í´ë˜ìŠ¤ êµ¬í˜„

```python
# tiny_audio/models/projector/custom.py

import torch
import torch.nn as nn

class CustomProjector(nn.Module):
    """ì»¤ìŠ¤í…€ Projector êµ¬í˜„"""

    def __init__(
        self,
        input_dim=1024,
        output_dim=896,
        # ì»¤ìŠ¤í…€ íŒŒë¼ë¯¸í„°ë“¤
        num_layers=3,
        attention_heads=8
    ):
        super().__init__()

        self.input_dim = input_dim
        self.output_dim = output_dim

        # ì»¤ìŠ¤í…€ ë ˆì´ì–´ êµ¬í˜„
        self.layers = nn.ModuleList([
            nn.TransformerEncoderLayer(
                d_model=input_dim * 4,
                nhead=attention_heads,
                dim_feedforward=input_dim * 4 * 4,
                batch_first=True
            )
            for _ in range(num_layers)
        ])

        # ì¶œë ¥ í”„ë¡œì ì…˜
        self.output_proj = nn.Linear(input_dim * 4, output_dim)

    def forward(self, audio_features):
        """
        Args:
            audio_features: [batch, seq_len, input_dim]

        Returns:
            text_embeddings: [batch, seq_len//4, output_dim]
        """
        # Frame stacking
        B, T, D = audio_features.shape
        audio_features = audio_features.reshape(B, T//4, D*4)

        # Transformer layers
        x = audio_features
        for layer in self.layers:
            x = layer(x)

        # Output projection
        text_embeddings = self.output_proj(x)

        return text_embeddings
```

### 2. ì„¤ì • íŒŒì¼ ì¶”ê°€

```yaml
# configs/experiments/custom.yaml
defaults:
  - override /data: multi_asr
  - override /training: default

model:
  projector_type: "custom"  # ìƒˆ íƒ€ì… ì´ë¦„
  projector_config:
    input_dim: 1024
    output_dim: 896
    num_layers: 3
    attention_heads: 8

training:
  learning_rate: 3e-4
  max_steps: 60000
```

### 3. Factoryì— ë“±ë¡

```python
# tiny_audio/models/projector/__init__.py

from .mlp import MLPProjector
from .mosa import MOSAProjector
from .moe import MoEProjector
from .qformer import QFormerProjector
from .custom import CustomProjector  # ì¶”ê°€

PROJECTOR_REGISTRY = {
    "mlp": MLPProjector,
    "mosa": MOSAProjector,
    "moe": MoEProjector,
    "qformer": QFormerProjector,
    "custom": CustomProjector,  # ë“±ë¡
}

def create_projector(projector_type, config):
    """Projector ìƒì„± factory"""
    if projector_type not in PROJECTOR_REGISTRY:
        raise ValueError(
            f"Unknown projector type: {projector_type}. "
            f"Available: {list(PROJECTOR_REGISTRY.keys())}"
        )

    projector_class = PROJECTOR_REGISTRY[projector_type]
    return projector_class(**config)
```

### 4. í›ˆë ¨ ì‹¤í–‰

```bash
ta train experiment=custom
```

## ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ ì¶”ê°€

ìì²´ ë°ì´í„°ì…‹ìœ¼ë¡œ í›ˆë ¨:

### 1. ë°ì´í„° ì¤€ë¹„

```
my_dataset/
â”œâ”€â”€ audio/
â”‚   â”œâ”€â”€ sample_001.wav
â”‚   â”œâ”€â”€ sample_002.wav
â”‚   â””â”€â”€ ...
â””â”€â”€ transcripts.json
```

`transcripts.json` í˜•ì‹:

```json
[
  {
    "audio_path": "audio/sample_001.wav",
    "text": "This is the transcription",
    "duration": 4.2,
    "speaker_id": "speaker_01",
    "metadata": {
      "domain": "medical",
      "quality": "clean"
    }
  },
  {
    "audio_path": "audio/sample_002.wav",
    "text": "Another transcription here",
    "duration": 6.8,
    "speaker_id": "speaker_02"
  }
]
```

### 2. Dataset í´ë˜ìŠ¤ êµ¬í˜„

```python
# tiny_audio/data/custom_dataset.py

import torch
from torch.utils.data import Dataset
import torchaudio
import json

class CustomDataset(Dataset):
    """ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹"""

    def __init__(
        self,
        data_dir,
        transcript_file,
        sample_rate=16000,
        max_audio_length=30,  # seconds
        split="train"
    ):
        self.data_dir = data_dir
        self.sample_rate = sample_rate
        self.max_audio_length = max_audio_length

        # ì „ì‚¬ íŒŒì¼ ë¡œë“œ
        with open(transcript_file) as f:
            self.samples = json.load(f)

        # Train/val ë¶„í•  (ì˜ˆ: 90/10)
        split_idx = int(len(self.samples) * 0.9)
        if split == "train":
            self.samples = self.samples[:split_idx]
        else:
            self.samples = self.samples[split_idx:]

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        sample = self.samples[idx]

        # ì˜¤ë””ì˜¤ ë¡œë“œ
        audio_path = f"{self.data_dir}/{sample['audio_path']}"
        waveform, sr = torchaudio.load(audio_path)

        # ë¦¬ìƒ˜í”Œë§
        if sr != self.sample_rate:
            resampler = torchaudio.transforms.Resample(sr, self.sample_rate)
            waveform = resampler(waveform)

        # ëª¨ë…¸ë¡œ ë³€í™˜
        if waveform.shape[0] > 1:
            waveform = waveform.mean(dim=0, keepdim=True)

        # ê¸¸ì´ ì œí•œ
        max_length = self.max_audio_length * self.sample_rate
        if waveform.shape[1] > max_length:
            waveform = waveform[:, :max_length]

        return {
            "audio": waveform.squeeze(0),
            "text": sample["text"],
            "audio_path": sample["audio_path"],
            "duration": sample.get("duration", 0),
        }
```

### 3. ì„¤ì • íŒŒì¼ ì¶”ê°€

```yaml
# configs/data/custom.yaml
name: custom_dataset
class_path: tiny_audio.data.CustomDataset

# ì´ˆê¸°í™” ì¸ì
init_args:
  data_dir: /path/to/my_dataset
  transcript_file: /path/to/my_dataset/transcripts.json
  sample_rate: 16000
  max_audio_length: 30

# DataLoader ì„¤ì •
dataloader:
  batch_size: 16
  num_workers: 4
  shuffle: true
  pin_memory: true
  drop_last: true

# ë°ì´í„° ì¦ê°• (ì„ íƒ)
augmentation:
  speed_perturbation: true
  noise_injection: true
  spec_augment: true
```

### 4. í›ˆë ¨ ì‹¤í–‰

```bash
ta train \
    experiment=transcription \
    data=custom
```

## RunPod ì›ê²© í›ˆë ¨

í´ë¼ìš°ë“œ GPUë¡œ í›ˆë ¨:

### 1. RunPod ì„¤ì •

```bash
# RunPod API í‚¤ ì„¤ì •
export RUNPOD_API_KEY=your_api_key_here

# RunPod CLI ì„¤ì¹˜
pip install runpod
```

### 2. ì›ê²© í›ˆë ¨ ì‹¤í–‰

```bash
ta runpod train \
    --experiment transcription \
    --gpu-type "NVIDIA A40" \
    --max-bid 0.50  # $/hour
```

ì‹¤í–‰ ê³¼ì •:

```
[INFO] Connecting to RunPod...
[INFO] Finding available GPU...
[INFO] Found: NVIDIA A40 (48GB) at $0.48/hour
[INFO] Starting pod...
[INFO] Pod ID: abc123def456
[INFO] Uploading code and data...
[INFO] Starting training...

[REMOTE] Step 1000/50000: loss=0.245, wer=28.3%
[REMOTE] Step 2000/50000: loss=0.198, wer=22.1%
...

[INFO] Training complete!
[INFO] Downloading checkpoints...
[INFO] Stopping pod...
[INFO] Total cost: $11.52
```

### 3. ì»¤ìŠ¤í…€ RunPod ìŠ¤í¬ë¦½íŠ¸

```python
# scripts/runpod_train.py

import runpod
import os

def setup_and_train():
    """RunPodì—ì„œ í›ˆë ¨ ì‹¤í–‰"""

    # í™˜ê²½ ì„¤ì •
    os.system("pip install -e .")

    # ë°ì´í„° ë‹¤ìš´ë¡œë“œ
    os.system("ta download-data multi_asr")

    # í›ˆë ¨ ì‹¤í–‰
    os.system("ta train experiment=transcription")

    # ì²´í¬í¬ì¸íŠ¸ ì—…ë¡œë“œ
    os.system("ta upload-checkpoints s3://my-bucket/checkpoints/")

if __name__ == "__main__":
    setup_and_train()
```

ì‹¤í–‰:

```bash
runpod create pod \
    --name "tiny-audio-training" \
    --gpu-type "NVIDIA A40" \
    --image pytorch/pytorch:2.0.0-cuda11.8-cudnn8-runtime \
    --script scripts/runpod_train.py \
    --volume /workspace:/data
```

## ê°œë°œ ë„êµ¬

ì½”ë“œ í’ˆì§ˆ ë„êµ¬:

### Lint

```bash
# Ruffë¡œ ë¦°íŒ…
ta lint

# ìë™ ìˆ˜ì •
ta lint --fix
```

### Format

```bash
# Blackìœ¼ë¡œ í¬ë§·íŒ…
ta format

# ì²´í¬ë§Œ
ta format --check
```

### Test

```bash
# ì „ì²´ í…ŒìŠ¤íŠ¸
ta test

# íŠ¹ì • íŒŒì¼
ta test tests/test_models.py

# ì»¤ë²„ë¦¬ì§€
ta test --coverage
```

### Pre-commit

```bash
# Pre-commit ì„¤ì¹˜
ta precommit install

# ìˆ˜ë™ ì‹¤í–‰
ta precommit run --all-files
```

`.pre-commit-config.yaml`:

```yaml
repos:
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.4.0
    hooks:
      - id: trailing-whitespace
      - id: end-of-file-fixer
      - id: check-yaml
      - id: check-added-large-files

  - repo: https://github.com/psf/black
    rev: 23.3.0
    hooks:
      - id: black

  - repo: https://github.com/charliermarsh/ruff-pre-commit
    rev: v0.0.270
    hooks:
      - id: ruff
        args: [--fix, --exit-non-zero-on-fix]
```

## í”„ë¡œë•ì…˜ ë°°í¬ ê°€ì´ë“œ

í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ ì•ˆì •ì ìœ¼ë¡œ ë°°í¬:

### Docker ì»¨í…Œì´ë„ˆ

```dockerfile
# Dockerfile
FROM pytorch/pytorch:2.0.0-cuda11.8-cudnn8-runtime

WORKDIR /app

# ì˜ì¡´ì„± ì„¤ì¹˜
COPY pyproject.toml poetry.lock ./
RUN pip install poetry && poetry install --no-dev

# ì½”ë“œ ë³µì‚¬
COPY . .

# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
RUN python -c "from transformers import pipeline; \
    pipeline('automatic-speech-recognition', \
    model='your-username/tiny-audio-custom', \
    trust_remote_code=True)"

# ì„œë²„ ì‹¤í–‰
EXPOSE 8000
CMD ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "8000"]
```

ë¹Œë“œ ë° ì‹¤í–‰:

```bash
# ë¹Œë“œ
docker build -t tiny-audio:latest .

# ì‹¤í–‰
docker run --gpus all -p 8000:8000 tiny-audio:latest
```

### FastAPI ì„œë²„

```python
# app.py
from fastapi import FastAPI, File, UploadFile
from transformers import pipeline
import tempfile
import os

app = FastAPI()

# ëª¨ë¸ ë¡œë“œ (ì‹œì‘ ì‹œ 1íšŒ)
pipe = pipeline(
    "automatic-speech-recognition",
    model="your-username/tiny-audio-custom",
    trust_remote_code=True,
    device="cuda"
)

@app.post("/transcribe")
async def transcribe(audio: UploadFile = File(...)):
    """ì˜¤ë””ì˜¤ íŒŒì¼ ì „ì‚¬"""

    # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
    with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp:
        content = await audio.read()
        tmp.write(content)
        tmp_path = tmp.name

    try:
        # ì „ì‚¬
        result = pipe(tmp_path, return_timestamps="word")

        return {
            "text": result["text"],
            "chunks": result["chunks"]
        }

    finally:
        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        os.unlink(tmp_path)

@app.get("/health")
async def health():
    """í—¬ìŠ¤ ì²´í¬"""
    return {"status": "healthy"}
```

ì‹¤í–‰:

```bash
uvicorn app:app --host 0.0.0.0 --port 8000 --workers 4
```

### Kubernetes ë°°í¬

```yaml
# deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: tiny-audio
spec:
  replicas: 3
  selector:
    matchLabels:
      app: tiny-audio
  template:
    metadata:
      labels:
        app: tiny-audio
    spec:
      containers:
      - name: tiny-audio
        image: your-registry/tiny-audio:latest
        ports:
        - containerPort: 8000
        resources:
          limits:
            nvidia.com/gpu: 1
            memory: "16Gi"
          requests:
            nvidia.com/gpu: 1
            memory: "8Gi"
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
---
apiVersion: v1
kind: Service
metadata:
  name: tiny-audio-service
spec:
  selector:
    app: tiny-audio
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8000
  type: LoadBalancer
```

ë°°í¬:

```bash
kubectl apply -f deployment.yaml
```

## ì„±ëŠ¥ ìµœì í™” íŒ

í”„ë¡œë•ì…˜ ì„±ëŠ¥ í–¥ìƒ:

### 1. ëª¨ë¸ ì–‘ìí™”

```python
from torch.quantization import quantize_dynamic

# ë™ì  ì–‘ìí™”
model_int8 = quantize_dynamic(
    model,
    {torch.nn.Linear},
    dtype=torch.qint8
)

# ì €ì¥
torch.save(model_int8.state_dict(), "model_int8.pt")
```

### 2. ONNX ë³€í™˜

```python
import torch.onnx

# ë”ë¯¸ ì…ë ¥
dummy_input = torch.randn(1, 16000 * 10)  # 10ì´ˆ

# ONNX ë‚´ë³´ë‚´ê¸°
torch.onnx.export(
    model,
    dummy_input,
    "tiny_audio.onnx",
    input_names=["audio"],
    output_names=["transcription"],
    dynamic_axes={
        "audio": {0: "batch", 1: "time"},
        "transcription": {0: "batch"}
    }
)
```

### 3. TensorRT ìµœì í™”

```bash
# ONNX â†’ TensorRT
trtexec \
    --onnx=tiny_audio.onnx \
    --saveEngine=tiny_audio.trt \
    --fp16
```

### 4. ë°°ì¹˜ ì²˜ë¦¬

```python
# ì—¬ëŸ¬ íŒŒì¼ì„ ë°°ì¹˜ë¡œ ì²˜ë¦¬
audio_files = ["file1.wav", "file2.wav", ..., "file16.wav"]
results = pipe(audio_files, batch_size=16)
```

## í–¥í›„ ë¡œë“œë§µ

Tiny Audioì˜ ê°œë°œ ê³„íš:

- [ ] ë‹¤êµ­ì–´ ì§€ì› (100+ ì–¸ì–´)
- [ ] ì‹¤ì‹œê°„ diarization
- [ ] Emotion recognition
- [ ] Code-switching ì§€ì›
- [ ] Edge ë””ë°”ì´ìŠ¤ ìµœì í™”
- [ ] Streaming fine-tuning
- [ ] Active learning

## ê¸°ì—¬ ë°©ë²•

í”„ë¡œì íŠ¸ì— ê¸°ì—¬í•˜ê¸°:

```bash
# 1. Fork & Clone
git clone https://github.com/your-username/tiny-audio.git
cd tiny-audio

# 2. ë¸Œëœì¹˜ ìƒì„±
git checkout -b feature/new-projector

# 3. ê°œë°œ
# ... ì½”ë“œ ì‘ì„± ...

# 4. í…ŒìŠ¤íŠ¸
ta test
ta lint
ta format

# 5. Commit & Push
git add .
git commit -m "Add new projector architecture"
git push origin feature/new-projector

# 6. Pull Request ìƒì„±
# GitHubì—ì„œ PR ìƒì„±
```

## ë¬´ë£Œ 3.5ì‹œê°„ ASR ì½”ìŠ¤

ë” ê¹Šì´ ë°°ìš°ê³  ì‹¶ë‹¤ë©´:

### ì½”ìŠ¤ ë‚´ìš©

1. **ASR ê¸°ì´ˆ** (30ë¶„)
   - ìŒì„± ì¸ì‹ ì—­ì‚¬
   - ì£¼ìš” ì ‘ê·¼ë²•
   - í‰ê°€ ë©”íŠ¸ë¦­

2. **ì˜¤ë””ì˜¤ ì²˜ë¦¬** (45ë¶„)
   - ì‹ í˜¸ ì²˜ë¦¬
   - íŠ¹ì§• ì¶”ì¶œ
   - ë°ì´í„° ì¦ê°•

3. **ëª¨ë¸ ì•„í‚¤í…ì²˜** (60ë¶„)
   - Encoder-Decoder
   - Attention ë©”ì»¤ë‹ˆì¦˜
   - Transformer

4. **í›ˆë ¨ ê¸°ë²•** (45ë¶„)
   - CTC Loss
   - Sequence-to-Sequence
   - Self-supervised learning

5. **í”„ë¡œë•ì…˜ ë°°í¬** (30ë¶„)
   - ìµœì í™”
   - ì„œë¹™
   - ëª¨ë‹ˆí„°ë§

### ë“±ë¡

ë¬´ë£Œ ì½”ìŠ¤ ë§í¬: [https://tiny-audio-course.com](https://github.com/alexkroman/tiny-audio)

## ì°¸ê³  ìë£Œ

- GitHub: [https://github.com/alexkroman/tiny-audio](https://github.com/alexkroman/tiny-audio)
- HuggingFace: [https://huggingface.co/alexkroman](https://huggingface.co/alexkroman)
- Pipecat: [https://github.com/pipecat-ai/pipecat](https://github.com/pipecat-ai/pipecat)
- RunPod: [https://www.runpod.io](https://www.runpod.io)
- WebRTC: [https://webrtc.org](https://webrtc.org)

## ë§ˆì¹˜ë©°

ì¶•í•˜í•©ë‹ˆë‹¤! Tiny Audioì˜ ëª¨ë“  ê¸°ëŠ¥ì„ ì‚´í´ë³´ì•˜ìŠµë‹ˆë‹¤.

ì´ì œ ì—¬ëŸ¬ë¶„ì€:
- ì»¤ìŠ¤í…€ ASR ëª¨ë¸ì„ í›ˆë ¨í•˜ê³ 
- ë‹¤ì–‘í•œ ë°©ë²•ìœ¼ë¡œ í‰ê°€í•˜ë©°
- HuggingFace Hubì— ë°°í¬í•˜ê³ 
- ì‹¤ì‹œê°„ ìŒì„± ì• í”Œë¦¬ì¼€ì´ì…˜ì— í†µí•©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤

ì§ˆë¬¸ì´ë‚˜ í”¼ë“œë°±ì€ GitHub Issuesì— ë‚¨ê²¨ì£¼ì„¸ìš”!
