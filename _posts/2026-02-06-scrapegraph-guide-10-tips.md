---
layout: post
title: "ScrapeGraphAI ì™„ë²½ ê°€ì´ë“œ (10) - ì‹¤ì „ í™œìš© ë° íŒ"
date: 2026-02-06
permalink: /scrapegraph-guide-10-tips/
author: ScrapeGraphAI Team
categories: [AI ë„êµ¬, ì›¹ ìŠ¤í¬ë˜í•‘]
tags: [ScrapeGraphAI, Best Practices, Tips, Optimization, Troubleshooting]
original_url: "https://github.com/ScrapeGraphAI/Scrapegraph-ai"
excerpt: "í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ ScrapeGraphAIë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì‚¬ìš©í•˜ê¸° ìœ„í•œ íŒê³¼ ë…¸í•˜ìš°ë¥¼ ê³µìœ í•©ë‹ˆë‹¤."
---

## í”„ë¡œë•ì…˜ ì²´í¬ë¦¬ìŠ¤íŠ¸

### 1. ì—ëŸ¬ í•¸ë“¤ë§

```python
from scrapegraphai.graphs import SmartScraperGraph
import logging

logging.basicConfig(level=logging.ERROR, filename='scraping_errors.log')

def safe_scrape(url, prompt, config, max_retries=3):
    """ì•ˆì „í•œ ìŠ¤í¬ë˜í•‘ ë˜í¼"""
    for attempt in range(max_retries):
        try:
            scraper = SmartScraperGraph(
                prompt=prompt,
                source=url,
                config=config
            )
            result = scraper.run()

            # ê²°ê³¼ ê²€ì¦
            if not result or len(result) == 0:
                raise ValueError("Empty result")

            return result

        except TimeoutError:
            logging.error(f"Timeout on {url}, attempt {attempt + 1}")
            if attempt == max_retries - 1:
                return {"error": "timeout", "url": url}

        except Exception as e:
            logging.error(f"Error on {url}: {e}")
            if attempt == max_retries - 1:
                return {"error": str(e), "url": url}

        time.sleep(2 ** attempt)  # ì§€ìˆ˜ ë°±ì˜¤í”„

    return None
```

### 2. í”„ë¡ì‹œ ë¡œí…Œì´ì…˜

```python
import random

PROXY_LIST = [
    "http://proxy1.com:8080",
    "http://proxy2.com:8080",
    "http://proxy3.com:8080",
    "http://proxy4.com:8080"
]

def get_random_proxy():
    return random.choice(PROXY_LIST)

def scrape_with_proxy(url, prompt):
    config = {
        "llm": {"model": "ollama/llama3.2"},
        "proxy": {"server": get_random_proxy()}
    }

    scraper = SmartScraperGraph(
        prompt=prompt,
        source=url,
        config=config
    )

    return scraper.run()
```

### 3. User-Agent ë¡œí…Œì´ì…˜

```python
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36"
]

def scrape_with_random_ua(url, prompt):
    config = {
        "llm": {"model": "ollama/llama3.2"},
        "loader_kwargs": {
            "user_agent": random.choice(USER_AGENTS)
        }
    }

    scraper = SmartScraperGraph(
        prompt=prompt,
        source=url,
        config=config
    )

    return scraper.run()
```

## ì„±ëŠ¥ ìµœì í™”

### 1. ë³‘ë ¬ ì²˜ë¦¬

```python
from concurrent.futures import ThreadPoolExecutor, as_completed
from scrapegraphai.graphs import SmartScraperGraph

urls = [f"https://example.com/page/{i}" for i in range(1, 101)]

def scrape_url(url):
    scraper = SmartScraperGraph(
        prompt="Extract data",
        source=url,
        config={"llm": {"model": "ollama/llama3.2"}}
    )
    return scraper.run()

# 10ê°œ ë™ì‹œ ì‹¤í–‰
with ThreadPoolExecutor(max_workers=10) as executor:
    futures = {executor.submit(scrape_url, url): url for url in urls}

    results = []
    for future in as_completed(futures):
        url = futures[future]
        try:
            result = future.result()
            results.append(result)
        except Exception as e:
            print(f"Error on {url}: {e}")
```

### 2. ìºì‹± ì „ëµ

```python
import hashlib
import json
import os

CACHE_DIR = "./cache"

def get_cache_key(url, prompt):
    """URLê³¼ í”„ë¡¬í”„íŠ¸ë¡œ ìºì‹œ í‚¤ ìƒì„±"""
    data = f"{url}:{prompt}"
    return hashlib.md5(data.encode()).hexdigest()

def get_cached_result(url, prompt):
    """ìºì‹œëœ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°"""
    cache_key = get_cache_key(url, prompt)
    cache_file = os.path.join(CACHE_DIR, f"{cache_key}.json")

    if os.path.exists(cache_file):
        with open(cache_file, 'r') as f:
            return json.load(f)
    return None

def save_to_cache(url, prompt, result):
    """ê²°ê³¼ë¥¼ ìºì‹œì— ì €ì¥"""
    cache_key = get_cache_key(url, prompt)
    cache_file = os.path.join(CACHE_DIR, f"{cache_key}.json")

    os.makedirs(CACHE_DIR, exist_ok=True)
    with open(cache_file, 'w') as f:
        json.dump(result, f)

def smart_scrape(url, prompt, config, use_cache=True):
    """ìºì‹œë¥¼ í™œìš©í•œ ìŠ¤í¬ë˜í•‘"""
    if use_cache:
        cached = get_cached_result(url, prompt)
        if cached:
            print(f"Cache hit for {url}")
            return cached

    # ìºì‹œ ë¯¸ìŠ¤: ì‹¤ì œ ìŠ¤í¬ë˜í•‘
    scraper = SmartScraperGraph(
        prompt=prompt,
        source=url,
        config=config
    )
    result = scraper.run()

    if use_cache:
        save_to_cache(url, prompt, result)

    return result
```

### 3. ë°°ì¹˜ ì²˜ë¦¬

```python
def batch_scrape(urls, prompt, batch_size=10):
    """ë°°ì¹˜ ë‹¨ìœ„ë¡œ ìŠ¤í¬ë˜í•‘"""
    results = []

    for i in range(0, len(urls), batch_size):
        batch = urls[i:i + batch_size]
        print(f"Processing batch {i//batch_size + 1}/{len(urls)//batch_size + 1}")

        batch_results = []
        for url in batch:
            try:
                result = smart_scrape(url, prompt, config)
                batch_results.append(result)
            except Exception as e:
                print(f"Error: {e}")
                batch_results.append({"error": str(e)})

        results.extend(batch_results)

        # ë°°ì¹˜ ê°„ ëŒ€ê¸° (Rate limit ë°©ì§€)
        time.sleep(2)

    return results
```

## ëª¨ë‹ˆí„°ë§ ë° ë¡œê¹…

### 1. êµ¬ì¡°í™”ëœ ë¡œê¹…

```python
import logging
import json
from datetime import datetime

class StructuredLogger:
    def __init__(self, log_file="scraping.log"):
        logging.basicConfig(
            level=logging.INFO,
            format='%(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)

    def log_scrape(self, url, prompt, result, duration):
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "url": url,
            "prompt": prompt,
            "success": "error" not in result,
            "duration_seconds": duration,
            "result_size": len(json.dumps(result))
        }
        self.logger.info(json.dumps(log_entry))

# ì‚¬ìš©
logger = StructuredLogger()

start = time.time()
result = smart_scrape(url, prompt, config)
duration = time.time() - start

logger.log_scrape(url, prompt, result, duration)
```

### 2. ë©”íŠ¸ë¦­ ìˆ˜ì§‘

```python
from collections import defaultdict

class ScrapeMetrics:
    def __init__(self):
        self.metrics = defaultdict(int)
        self.durations = []

    def record_success(self, duration):
        self.metrics["success"] += 1
        self.durations.append(duration)

    def record_failure(self, error_type):
        self.metrics[f"error_{error_type}"] += 1

    def get_summary(self):
        total = sum(self.metrics.values())
        success_rate = (self.metrics["success"] / total * 100) if total > 0 else 0
        avg_duration = sum(self.durations) / len(self.durations) if self.durations else 0

        return {
            "total_requests": total,
            "success_rate": f"{success_rate:.2f}%",
            "avg_duration": f"{avg_duration:.2f}s",
            "errors": {k: v for k, v in self.metrics.items() if k.startswith("error_")}
        }

# ì‚¬ìš©
metrics = ScrapeMetrics()

for url in urls:
    start = time.time()
    try:
        result = smart_scrape(url, prompt, config)
        duration = time.time() - start
        metrics.record_success(duration)
    except TimeoutError:
        metrics.record_failure("timeout")
    except Exception as e:
        metrics.record_failure("other")

print(metrics.get_summary())
```

## í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ íŒ

### 1. êµ¬ì²´ì ì¸ ì¶œë ¥ êµ¬ì¡° ì§€ì •

```python
prompt = """
Extract product information in the following JSON format:
{
    "name": "product name",
    "price": {
        "amount": 99.99,
        "currency": "USD"
    },
    "rating": {
        "average": 4.5,
        "count": 120
    },
    "availability": "in_stock" or "out_of_stock",
    "features": ["feature1", "feature2"]
}

Return ONLY valid JSON, no additional text.
"""
```

### 2. Few-Shot ì˜ˆì œ ì œê³µ

```python
prompt = """
Extract article metadata.

Example 1:
Input: <article><h1>AI News</h1><p>By John Doe</p></article>
Output: {"title": "AI News", "author": "John Doe"}

Example 2:
Input: <article><h1>Tech Update</h1><p>By Jane Smith</p></article>
Output: {"title": "Tech Update", "author": "Jane Smith"}

Now extract from the following HTML:
"""
```

### 3. ì¡°ê±´ë¶€ ì¶”ì¶œ

```python
prompt = """
Extract product information:
- If price includes discount, extract both original and discounted price
- If product is unavailable, set availability to "out_of_stock"
- If rating is not shown, omit the rating field
- Extract up to 5 main features
"""
```

## ë°ì´í„° í’ˆì§ˆ ê²€ì¦

### 1. ìŠ¤í‚¤ë§ˆ ê²€ì¦

```python
from jsonschema import validate, ValidationError

PRODUCT_SCHEMA = {
    "type": "object",
    "properties": {
        "name": {"type": "string"},
        "price": {"type": "number"},
        "stock": {"type": "string", "enum": ["in_stock", "out_of_stock"]}
    },
    "required": ["name", "price"]
}

def validate_result(result):
    try:
        validate(instance=result, schema=PRODUCT_SCHEMA)
        return True
    except ValidationError as e:
        print(f"Validation error: {e.message}")
        return False

# ì‚¬ìš©
result = smart_scrape(url, prompt, config)
if validate_result(result):
    print("Valid data")
else:
    print("Invalid data, re-scraping...")
```

### 2. í•„ë“œ ê²€ì¦

```python
def validate_product_data(data):
    """ì œí’ˆ ë°ì´í„° ê²€ì¦"""
    errors = []

    # í•„ìˆ˜ í•„ë“œ í™•ì¸
    if "name" not in data or not data["name"]:
        errors.append("Missing or empty name")

    # ê°€ê²© ë²”ìœ„ í™•ì¸
    if "price" in data:
        try:
            price = float(data["price"])
            if price < 0 or price > 100000:
                errors.append(f"Unrealistic price: ${price}")
        except ValueError:
            errors.append("Invalid price format")

    # ì´ë©”ì¼ í˜•ì‹ í™•ì¸
    if "email" in data:
        if "@" not in data["email"]:
            errors.append("Invalid email format")

    return len(errors) == 0, errors

# ì‚¬ìš©
result = smart_scrape(url, prompt, config)
valid, errors = validate_product_data(result)

if not valid:
    print(f"Validation errors: {errors}")
```

## ë¹„ìš© ìµœì í™”

### 1. ì ì‘í˜• ëª¨ë¸ ì„ íƒ

```python
def adaptive_scrape(url, prompt, complexity="auto"):
    """í˜ì´ì§€ ë³µì¡ë„ì— ë”°ë¼ ëª¨ë¸ ì„ íƒ"""

    if complexity == "auto":
        # ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±ìœ¼ë¡œ ë³µì¡ë„ íŒë‹¨
        if len(prompt) < 50 and "simple" in url:
            complexity = "simple"
        else:
            complexity = "complex"

    if complexity == "simple":
        # ì €ë ´í•œ ëª¨ë¸
        config = {"llm": {"model": "ollama/llama3.2"}}
    else:
        # ê³ ì„±ëŠ¥ ëª¨ë¸
        config = {"llm": {"model": "openai/gpt-4o-mini", "api_key": "sk-..."}}

    scraper = SmartScraperGraph(
        prompt=prompt,
        source=url,
        config=config
    )

    return scraper.run()
```

### 2. í† í° ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§

```python
from tiktoken import encoding_for_model

def estimate_cost(prompt, result, model="gpt-4o-mini"):
    """ë¹„ìš© ì¶”ì •"""
    enc = encoding_for_model(model)

    prompt_tokens = len(enc.encode(prompt))
    output_tokens = len(enc.encode(json.dumps(result)))

    # GPT-4o-mini ê°€ê²©
    input_cost = prompt_tokens / 1_000_000 * 0.15
    output_cost = output_tokens / 1_000_000 * 0.60

    total_cost = input_cost + output_cost

    return {
        "prompt_tokens": prompt_tokens,
        "output_tokens": output_tokens,
        "total_cost": f"${total_cost:.6f}"
    }
```

## ì¼ë°˜ì ì¸ ë¬¸ì œ í•´ê²°

### 1. JavaScript ë Œë”ë§ ë¬¸ì œ

```python
config = {
    "llm": {"model": "ollama/llama3.2"},
    "loader_kwargs": {
        "wait_until": "networkidle",  # ë„¤íŠ¸ì›Œí¬ ìœ íœ´ ëŒ€ê¸°
        "timeout": 60000,              # ì¶©ë¶„í•œ íƒ€ì„ì•„ì›ƒ
        "wait_for_selector": ".content"  # íŠ¹ì • ìš”ì†Œ ëŒ€ê¸°
    }
}
```

### 2. CAPTCHA ìš°íšŒ

```python
# Undetected Playwright ì‚¬ìš©
config = {
    "llm": {"model": "ollama/llama3.2"},
    "use_undetected_playwright": True,  # Anti-bot ìš°íšŒ
}
```

### 3. Rate Limit ì²˜ë¦¬

```python
import time
from ratelimit import limits, sleep_and_retry

CALLS = 10
PERIOD = 60  # seconds

@sleep_and_retry
@limits(calls=CALLS, period=PERIOD)
def rate_limited_scrape(url, prompt, config):
    scraper = SmartScraperGraph(
        prompt=prompt,
        source=url,
        config=config
    )
    return scraper.run()
```

## ë³´ì•ˆ ê³ ë ¤ì‚¬í•­

### 1. API í‚¤ ë³´í˜¸

```python
# âŒ ë‚˜ìœ ì˜ˆ: ì½”ë“œì— í•˜ë“œì½”ë”©
config = {"llm": {"api_key": "sk-proj-abc123..."}}

# âœ… ì¢‹ì€ ì˜ˆ: í™˜ê²½ ë³€ìˆ˜ ì‚¬ìš©
import os
config = {"llm": {"api_key": os.getenv("OPENAI_API_KEY")}}
```

### 2. ë¯¼ê° ë°ì´í„° ë§ˆìŠ¤í‚¹

```python
import re

def mask_sensitive_data(result):
    """ë¯¼ê° ë°ì´í„° ë§ˆìŠ¤í‚¹"""
    result_str = json.dumps(result)

    # ì´ë©”ì¼ ë§ˆìŠ¤í‚¹
    result_str = re.sub(
        r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',
        '***@***.***',
        result_str
    )

    # ì „í™”ë²ˆí˜¸ ë§ˆìŠ¤í‚¹
    result_str = re.sub(
        r'\b\d{3}[-.]?\d{3}[-.]?\d{4}\b',
        '***-***-****',
        result_str
    )

    return json.loads(result_str)
```

## ê²°ë¡ 

ScrapeGraphAIëŠ” ê°•ë ¥í•˜ê³  ìœ ì—°í•œ ì›¹ ìŠ¤í¬ë˜í•‘ ë„êµ¬ì…ë‹ˆë‹¤. ì´ ê°€ì´ë“œì—ì„œ ë‹¤ë£¬ ë‚´ìš©ì„ ìš”ì•½í•˜ë©´:

1. **LLM ê¸°ë°˜**: ìì—°ì–´ í”„ë¡¬í”„íŠ¸ë¡œ ìŠ¤í¬ë˜í•‘
2. **ë‹¤ì–‘í•œ ê·¸ë˜í”„**: ë‹¨ì¼/ë©€í‹° í˜ì´ì§€, ê²€ìƒ‰, ë¬¸ì„œ ë“±
3. **ë©€í‹° í¬ë§·**: JSON, CSV, XML, PDF ì§€ì›
4. **ê³ ê¸‰ ê¸°ëŠ¥**: ì½”ë“œ ìƒì„±, ìŠ¤í¬ë¦½íŠ¸ ìƒì„±, ìŒì„± ë³€í™˜
5. **í­ë„“ì€ í†µí•©**: API, SDK, Langchain, n8n, Zapier

### ì¶”ê°€ ë¦¬ì†ŒìŠ¤

- [ê³µì‹ ë¬¸ì„œ](https://scrapegraph-ai.readthedocs.io/)
- [GitHub ì €ì¥ì†Œ](https://github.com/ScrapeGraphAI/Scrapegraph-ai)
- [Discord ì»¤ë®¤ë‹ˆí‹°](https://discord.gg/gkxQDAjfeX)
- [API ëŒ€ì‹œë³´ë“œ](https://dashboard.scrapegraphai.com/)

---

## ì‹œë¦¬ì¦ˆ ë„¤ë¹„ê²Œì´ì…˜

- **ì´ì „**: [(9) í†µí•© ë° í™•ì¥]({{ site.baseurl }}/scrapegraph-guide-09-integrations/)
- **í˜„ì¬**: (10) ì‹¤ì „ í™œìš© ë° íŒ

[ğŸ“š ì „ì²´ ëª©ì°¨ë¡œ ëŒì•„ê°€ê¸°]({{ site.baseurl }}/scrapegraph-guide/)

---

ì´ê²ƒìœ¼ë¡œ **ScrapeGraphAI ì™„ë²½ ê°€ì´ë“œ** ì‹œë¦¬ì¦ˆë¥¼ ë§ˆì¹©ë‹ˆë‹¤. ì—¬ëŸ¬ë¶„ì˜ í”„ë¡œì íŠ¸ì— ScrapeGraphAIë¥¼ ì„±ê³µì ìœ¼ë¡œ ì ìš©í•˜ì‹œê¸¸ ë°”ëë‹ˆë‹¤!
