---
layout: page
title: Tiny Audio ê°€ì´ë“œ
permalink: /tiny-audio-guide/
icon: fas fa-microphone
---

# Tiny Audio ì™„ë²½ ê°€ì´ë“œ

> **24ì‹œê°„ì— $12ë¡œ ìŒì„± ì¸ì‹ ëª¨ë¸ í›ˆë ¨í•˜ê¸°**

**Tiny Audio**ëŠ” ìµœì†Œí•œì˜ ì½”ë“œë¡œ ASR(ìë™ ìŒì„± ì¸ì‹) ëª¨ë¸ì„ êµ¬ì¶•í•˜ê³  í›ˆë ¨í•  ìˆ˜ ìˆëŠ” ë¯¸ë‹ˆë©€í•œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. Frozen audio encoderì™€ Frozen LLMì„ ì†Œí˜• Projectorë¡œ ì—°ê²°í•˜ì—¬ íš¨ìœ¨ì ìœ¼ë¡œ í•™ìŠµí•©ë‹ˆë‹¤.

---

## ëª©ì°¨

| # | ì œëª© | ë‚´ìš© |
|---|------|------|
| 01 | [ì†Œê°œ ë° ê°œìš”](/blog-repo/tiny-audio-guide-01-intro/) | Tiny Audioë€?, í•µì‹¬ ì•„ì´ë””ì–´, ì•„í‚¤í…ì²˜ ê°œìš” |
| 02 | [ì„¤ì¹˜ ë° ë¹ ë¥¸ ì‹œì‘](/blog-repo/tiny-audio-guide-02-quick-start/) | ì„¤ì¹˜, ê¸°ë³¸ ì¶”ë¡ , ìŠ¤íŠ¸ë¦¬ë°, Timestamps |
| 03 | [ì•„í‚¤í…ì²˜ ìƒì„¸](/blog-repo/tiny-audio-guide-03-architecture/) | 3ê°€ì§€ ì»´í¬ë„ŒíŠ¸, 4ê°€ì§€ Projector íƒ€ì… |
| 04 | [ëª¨ë¸ í›ˆë ¨](/blog-repo/tiny-audio-guide-04-training/) | Hydra ì„¤ì •, 4ê°€ì§€ ì‹¤í—˜, 3-Stage LoRA í›ˆë ¨ |
| 05 | [í‰ê°€ ë° ë¶„ì„](/blog-repo/tiny-audio-guide-05-evaluation/) | CLI í‰ê°€, WER ë¶„ì„, ëª¨ë¸ ë¹„êµ |
| 06 | [ë°°í¬ ë° í™•ì¥](/blog-repo/tiny-audio-guide-06-deployment/) | HuggingFace ë°°í¬, Voice Agent, ì»¤ìŠ¤í…€ í™•ì¥ |

---

## ì£¼ìš” íŠ¹ì§•

- **ğŸ’° ì €ë¹„ìš© í›ˆë ¨** - A40 GPUë¡œ 24ì‹œê°„, $12ì— í›ˆë ¨ ê°€ëŠ¥
- **ğŸ”§ ë¯¸ë‹ˆë©€ & í•´í‚¹ ê°€ëŠ¥** - í•µì‹¬ ì½”ë“œë§Œ í¬í•¨, ì‰½ê²Œ ìˆ˜ì • ê°€ëŠ¥
- **â„ï¸ Frozen Architecture** - Encoderì™€ LLMì€ ë™ê²°, Projectorë§Œ í›ˆë ¨ (~12M params)
- **ğŸš€ ë¹ ë¥¸ ì¶”ë¡ ** - HuggingFace Pipeline, ìŠ¤íŠ¸ë¦¬ë°, Word-level timestamps
- **ğŸ“ êµìœ¡ ì¹œí™”ì ** - ë¬´ë£Œ 3.5ì‹œê°„ ASR ì½”ìŠ¤ í¬í•¨
- **ğŸ”Œ í™•ì¥ ê°€ëŠ¥** - 4ê°€ì§€ Projector íƒ€ì…, ì»¤ìŠ¤í…€ í™•ì¥ ì§€ì›

---

## ë¹ ë¥¸ ì‹œì‘

### ì„¤ì¹˜

```bash
# Poetryë¡œ ì„¤ì¹˜ (ê¶Œì¥)
git clone https://github.com/alexkroman/tiny-audio.git
cd tiny-audio
poetry install

# PyPIì—ì„œ ì„¤ì¹˜ (ì¶”ë¡ ë§Œ)
pip install tiny-audio
```

### ê¸°ë³¸ ì‚¬ìš©

```python
from transformers import pipeline

# ëª¨ë¸ ë¡œë“œ
pipe = pipeline(
    "automatic-speech-recognition",
    model="mazesmazes/tiny-audio",
    trust_remote_code=True
)

# ì˜¤ë””ì˜¤ íŒŒì¼ ë³€í™˜
result = pipe("audio.wav")
print(result["text"])

# Word-level timestamps
result = pipe("audio.wav", return_timestamps="word")
print(result["chunks"])  # [{"text": "hello", "start": 0.0, "end": 0.5}, ...]
```

### ëª¨ë¸ í›ˆë ¨

```bash
# ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (~5ë¶„)
poetry run python scripts/train.py \
    +experiments=transcription \
    data.max_train_samples=100 \
    training.max_steps=10

# ì „ì²´ í›ˆë ¨ (~24ì‹œê°„, $12)
poetry run python scripts/train.py +experiments=transcription
```

---

## ì•„í‚¤í…ì²˜ ê°œìš”

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       Tiny Audio Architecture                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                   â”‚
â”‚  Audio (16kHz)                                                   â”‚
â”‚       â†“                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚  GLM-ASR Encoder (~600M params)      [FROZEN â„ï¸]   â”‚        â”‚
â”‚  â”‚  - Frame-level audio embeddings                      â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚       â†“                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚  MLP Projector (~12M params)         [TRAINED ğŸ”¥]   â”‚        â”‚
â”‚  â”‚  - Modality bridge: audio â†’ text space               â”‚        â”‚
â”‚  â”‚  - Frame stacking for sequence reduction             â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚       â†“                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚  Qwen3-0.6B LLM (~600M params)       [FROZEN â„ï¸]   â”‚        â”‚
â”‚  â”‚  - Text generation from audio features               â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚       â†“                                                          â”‚
â”‚  Text Output                                                     â”‚
â”‚                                                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### í•µì‹¬ ì•„ì´ë””ì–´

**Only the Projector trains!** ğŸ¯

- **Encoder (Frozen)**: GLM-ASRê°€ ì˜¤ë””ì˜¤ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
- **Projector (Trained)**: ì˜¤ë””ì˜¤ ê³µê°„ â†’ í…ìŠ¤íŠ¸ ê³µê°„ ë§¤í•‘ (~12M paramsë§Œ í•™ìŠµ)
- **LLM (Frozen)**: Qwen3ê°€ ë³€í™˜ëœ ì„ë² ë”©ìœ¼ë¡œ í…ìŠ¤íŠ¸ ìƒì„±

â†’ **ì „ì²´ 1.2B params ì¤‘ 1% (12M)ë§Œ í›ˆë ¨!** âš¡

---

## Projector íƒ€ì… ë¹„êµ

| Projector | Params | êµ¬ì¡° | ì¥ì  | ë‹¨ì  |
|-----------|--------|------|------|------|
| **MLP** | ~12M | Simple 2-layer MLP | ë¹ ë¥¸ í›ˆë ¨, ë‚®ì€ ë©”ëª¨ë¦¬ | ê¸°ë³¸ ì„±ëŠ¥ |
| **MOSA** | ~16M | Dense MoE | íŒŒë¼ë¯¸í„° ê³µìœ  íš¨ìœ¨ì  | ì•½ê°„ ëŠë¦¼ |
| **MoE** | ~24M | Sparse Experts | ë†’ì€ í‘œí˜„ë ¥ | ë” ë§ì€ íŒŒë¼ë¯¸í„° |
| **QFormer** | ~18M | Transformer Queries | ìœ ì—°í•œ ë§¤í•‘ | í›ˆë ¨ ë³µì¡ë„ ë†’ìŒ |

---

## 3-Stage LoRA í›ˆë ¨

```bash
# Stage 1: Projectorë§Œ í›ˆë ¨ (ê¸°ë³¸)
poetry run python scripts/train.py +experiments=transcription
# â†’ WER: 5.5%, Params: 12M

# Stage 2: LoRA ì–´ëŒ‘í„° ì¶”ê°€
poetry run python scripts/train.py +experiments=mlp_lora
# â†’ WER: 4.8%, Params: +4.2M

# Stage 3: Projector + LoRA Fine-tune
poetry run python scripts/train.py +experiments=mlp_fine_tune
# â†’ WER: 4.5%, Params: 16.2M
```

---

## CLI ë„êµ¬

```bash
# í‰ê°€
poetry run ta eval -m mazesmazes/tiny-audio -n 100

# WER ë¶„ì„
poetry run ta analysis high-wer mazesmazes/tiny-audio --threshold 30
poetry run ta analysis compare model1 model2

# ë°°í¬
poetry run ta push my-model             # HuggingFace Hubì— í‘¸ì‹œ
poetry run ta deploy my-space           # Space ë°°í¬
poetry run ta demo                      # ë¡œì»¬ Gradio ë°ëª¨

# ê°œë°œ
poetry run ta dev lint                  # ì½”ë“œ Lint
poetry run ta dev format                # ì½”ë“œ í¬ë§·
poetry run ta dev test                  # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
```

---

## ë°ì´í„°ì…‹ & ì„±ëŠ¥

### í›ˆë ¨ ë°ì´í„°

- **Multi-ASR Dataset**: ë‹¤ì–‘í•œ ASR ë°ì´í„°ì…‹ ì¡°í•©
- **LoquaciousSet**: ê³ í’ˆì§ˆ ìŒì„± ë°ì´í„°
- **ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹** ì¶”ê°€ ê°€ëŠ¥

### ë²¤ì¹˜ë§ˆí¬ (WER %)

| ëª¨ë¸ | LibriSpeech-test-clean | í›ˆë ¨ ë¹„ìš© | í›ˆë ¨ ì‹œê°„ |
|------|----------------------|---------|---------|
| Whisper-tiny | 5.4% | N/A | N/A |
| **Tiny Audio (MLP)** | 5.5% | **$12** | **24h** |
| Tiny Audio (MoE) | 5.1% | $15 | 28h |

---

## Voice Agent í†µí•©

```python
from tiny_audio.integrations import PipecatASRService

# Pipecat-AIì™€ í†µí•©
asr = PipecatASRService(model="mazesmazes/tiny-audio")

# WebRTC ìŠ¤íŠ¸ë¦¬ë°
async for transcript in asr.stream(audio_chunks):
    print(transcript.text)

# OpenAI Realtime API ëŒ€ì²´
```

---

## í”„ë¡œì íŠ¸ êµ¬ì¡°

```
tiny-audio/
â”œâ”€â”€ tiny_audio/              # í•µì‹¬ ë¼ì´ë¸ŒëŸ¬ë¦¬
â”‚   â”œâ”€â”€ asr_modeling.py      # ASRModel: encoder + projector + decoder
â”‚   â”œâ”€â”€ asr_config.py        # ASRConfig: ì„¤ì •
â”‚   â”œâ”€â”€ asr_pipeline.py      # HuggingFace Pipeline
â”‚   â”œâ”€â”€ asr_processing.py    # Processor: ì „ì²˜ë¦¬
â”‚   â”œâ”€â”€ projectors.py        # 4ê°€ì§€ Projector êµ¬í˜„
â”‚   â””â”€â”€ integrations/        # Voice agent í†µí•©
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train.py             # í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸ (Hydra)
â”‚   â”œâ”€â”€ cli.py               # CLI ì§„ì…ì  (ta)
â”‚   â”œâ”€â”€ eval/                # í‰ê°€ í”„ë ˆì„ì›Œí¬
â”‚   â”œâ”€â”€ analysis.py          # WER ë¶„ì„
â”‚   â”œâ”€â”€ deploy/              # HF Space ë°°í¬
â”‚   â””â”€â”€ debug/               # ë””ë²„ê·¸ ë„êµ¬
â”œâ”€â”€ configs/                 # Hydra ì„¤ì •
â”‚   â”œâ”€â”€ config.yaml          # ë©”ì¸ ì„¤ì •
â”‚   â”œâ”€â”€ experiments/         # Projector í”„ë¦¬ì…‹
â”‚   â”œâ”€â”€ data/                # ë°ì´í„°ì…‹ ì„¤ì •
â”‚   â””â”€â”€ training/            # í›ˆë ¨ í•˜ì´í¼íŒŒë¼ë¯¸í„°
â””â”€â”€ docs/                    # ë¬¸ì„œ ë° ì½”ìŠ¤
    â”œâ”€â”€ QUICKSTART.md        # ë¹ ë¥¸ ì‹œì‘ ê°€ì´ë“œ
    â””â”€â”€ course/              # ë¬´ë£Œ 3.5ì‹œê°„ ASR ì½”ìŠ¤
```

---

## ì»¤ìŠ¤í…€ í™•ì¥

### Projector ì¶”ê°€

```python
# tiny_audio/projectors.py
class MyProjector(nn.Module):
    def __init__(self, input_dim: int, output_dim: int, config):
        super().__init__()
        # ì»¤ìŠ¤í…€ ì•„í‚¤í…ì²˜ êµ¬í˜„

    def forward(self, x, attention_mask=None):
        return projected_features

    def get_output_length(self, input_length: int) -> int:
        return output_length

# PROJECTOR_CLASSESì— ë“±ë¡
PROJECTOR_CLASSES["my_projector"] = MyProjector
```

### ë°ì´í„°ì…‹ ì¶”ê°€

```yaml
# configs/data/my_dataset.yaml
dataset_name: "your-org/your-dataset"
dataset_split: "train"
audio_column: "audio"
text_column: "text"
```

```bash
# í›ˆë ¨
poetry run python scripts/train.py data=my_dataset
```

---

## í•™ìŠµ ë¦¬ì†ŒìŠ¤

### ë¬´ë£Œ 3.5ì‹œê°„ ASR ì½”ìŠ¤

Tiny AudioëŠ” ASRì„ ì²˜ìŒë¶€í„° êµ¬ì¶•í•˜ëŠ” ë¬´ë£Œ ì½”ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤:

```
docs/course/
â”œâ”€â”€ 0-course-overview.md
â”œâ”€â”€ 1-audio-preprocessing.md
â”œâ”€â”€ 2-feature-extraction.md
â”œâ”€â”€ 3-encoder-models.md
â”œâ”€â”€ 4-projector-design.md
â”œâ”€â”€ 5-decoder-integration.md
â””â”€â”€ 6-training-evaluation.md
```

**ì½”ìŠ¤ ë§í¬**: [docs/course/0-course-overview.md](https://github.com/alexkroman/tiny-audio/tree/main/docs/course)

---

## í”„ë¡œë•ì…˜ ë°°í¬

### Docker ì»¨í…Œì´ë„ˆ

```dockerfile
FROM pytorch/pytorch:2.0.0-cuda11.8-cudnn8-runtime
RUN pip install tiny-audio
COPY app.py /app/
CMD ["python", "/app/app.py"]
```

### FastAPI ì„œë²„

```python
from fastapi import FastAPI, File, UploadFile
from tiny_audio import ASRModel, ASRProcessor

app = FastAPI()
model = ASRModel.from_pretrained("mazesmazes/tiny-audio")
processor = ASRProcessor.from_pretrained("mazesmazes/tiny-audio")

@app.post("/transcribe")
async def transcribe(file: UploadFile = File(...)):
    audio = await file.read()
    result = model.transcribe(audio)
    return {"text": result}
```

### Kubernetes ë°°í¬

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: tiny-audio
spec:
  replicas: 3
  template:
    spec:
      containers:
      - name: asr
        image: your-registry/tiny-audio:latest
        resources:
          limits:
            nvidia.com/gpu: 1
```

---

## ì„±ëŠ¥ ìµœì í™”

### ì–‘ìí™”

```python
# INT8 ì–‘ìí™”
model = ASRModel.from_pretrained(
    "mazesmazes/tiny-audio",
    quantization_config={"load_in_8bit": True}
)
```

### ONNX ë³€í™˜

```bash
# ONNXë¡œ ë³€í™˜ (ì¶”ë¡  2-3ë°° ì†ë„ í–¥ìƒ)
poetry run python scripts/export_onnx.py --model mazesmazes/tiny-audio
```

### TensorRT

```bash
# TensorRT ìµœì í™” (NVIDIA GPU)
poetry run python scripts/export_tensorrt.py --model mazesmazes/tiny-audio
```

---

## í™˜ê²½ ë³€ìˆ˜

| ë³€ìˆ˜ | ì„¤ëª… |
|------|------|
| `HF_TOKEN` | HuggingFace API í† í° (private ëª¨ë¸/í‘¸ì‹œìš©) |
| `WANDB_API_KEY` | Weights & Biases API í‚¤ |
| `WANDB_RUN_ID` | íŠ¹ì • W&B ì‹¤í–‰ ì¬ê°œ |
| `ASSEMBLYAI_API_KEY` | AssemblyAI í‰ê°€ ë¹„êµìš© |

---

## ë¼ì´ì„ ìŠ¤ ë° ì¸ìš©

**ë¼ì´ì„ ìŠ¤**: MIT License

**ê°ì‚¬ì˜ ë§**:
- [GLM-ASR](https://huggingface.co/zai-org/GLM-ASR-Nano-2512) - ì˜¤ë””ì˜¤ ì¸ì½”ë”
- [Qwen3](https://huggingface.co/Qwen/Qwen3-0.6B) - ì–¸ì–´ ëª¨ë¸
- [LoquaciousSet](https://huggingface.co/datasets/speechbrain/LoquaciousSet) - í›ˆë ¨ ë°ì´í„°

---

## ê´€ë ¨ ë§í¬

- [GitHub ì €ì¥ì†Œ](https://github.com/alexkroman/tiny-audio)
- [HuggingFace ëª¨ë¸](https://huggingface.co/mazesmazes/tiny-audio)
- [Live Demo](https://huggingface.co/spaces/mazesmazes/tiny-audio)
- [ë¹ ë¥¸ ì‹œì‘ ê°€ì´ë“œ](https://github.com/alexkroman/tiny-audio/blob/main/docs/QUICKSTART.md)
- [ë¬´ë£Œ ASR ì½”ìŠ¤](https://github.com/alexkroman/tiny-audio/tree/main/docs/course)
- [ëª¨ë¸ ì¹´ë“œ](https://github.com/alexkroman/tiny-audio/blob/main/MODEL_CARD.md)

---

*ì‘ì„±ì¼: 2026ë…„ 2ì›” 9ì¼*
*ì €ì: Alex Kroman*
